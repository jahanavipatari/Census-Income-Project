# Census-Income-Project
The dataset we are going to use is the Census income dataset from Kaggle, which contains about 32,561 rows and 15 features.  The dataset includes labels that we need to predict, and the labels are discrete and binary. Therefore, the problem is a supervised classification type.

# Motivation
Building predictive models like this one can help us better understand the population of a country and the various factors influencing economic growth. By analyzing these factors, governments can make informed decisions to improve them, leading to the country's development and prosperity. These insights can also be beneficial to businesses and organizations, allowing them to target specific demographic groups and make data-driven decisions about their operations and marketing strategies. Ultimately, a deeper understanding of income distribution and the factors that drive it can contribute to better resource allocation and policies aimed at reducing inequality and promoting economic stability.

# Conclusion
In this project, we developed and tested several machine learning models, including logistic regression, KNN classifier, support vector classifier, decision tree classifier, random forest classifier, and XGBoost classifier. After tuning the hyperparameters, the random forest classifier achieved the highest accuracy score of 92.77% and an F1 score of 93.08%. This demonstrates the effectiveness of ensemble learning techniques in handling complex classification tasks. The decision tree and KNN models also showed promising results, but they were slightly outperformed by the random forest classifier. Overall, the models were able to classify income levels accurately, making them valuable tools for predicting income distribution in various contexts.

# Future Work
Given that the dataset is large enough, we can explore using neural networks, such as artificial neural networks (ANNs), to potentially improve model performance even further. Neural networks have been shown to handle large, complex datasets well and could capture more intricate patterns that traditional models might miss. Additionally, we could experiment with feature engineering to extract more meaningful information from the existing data, or explore other advanced techniques like deep learning, gradient boosting, or reinforcement learning. We could also expand the dataset with more demographic and economic features to further enhance model accuracy and applicability. Exploring these avenues would help refine our model and make it even more robust for real-world applications.
